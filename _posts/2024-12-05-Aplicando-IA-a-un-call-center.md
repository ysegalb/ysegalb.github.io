---
layout: post
title: "Aplicando IA a un call center"
date: 2024-12-05
categories: [tutorial, IA]
---

## ¬øPor qu√© esto no son otros calcetines con IA?

Hoy en d√≠a, hemos asistido a la explosi√≥n de la IA y, en particular, de los LLM generativos como ChatGPT. Es dif√≠cil encontrar un producto que, habiendo sido actualizado o lanzado en el √∫ltimo a√±o, no lleve alguna referencia a la IA. Adem√°s, habiendo vivido otras revoluciones similares, sabemos que en muchas ocasiones se fuerzan los casos de uso para decir que est√°n hechos con Inteligencia Artificial. Esto ha generado una saturaci√≥n en el mercado e, incluso, a veces, cierto recelo cuando vemos las consabidas siglas.

¬øQuiere decir eso que su aplicaci√≥n no es √∫til? Para nada. Hay casos que, si bien pueden no ser novedosos en cuanto al planteamiento, se han convertido en soluciones mucho m√°s asequibles, tanto para desarrollar como para mantener. Y uno de esos casos es el que vamos a presentar en este tutorial.

Si alguna vez hab√©is tenido relaci√≥n con un centro de atenci√≥n al cliente, sabr√©is que todas las conversaciones son almacenadas, ya sea en forma de audios o como chats con los agentes. Esto es necesario para aportar trazabilidad a los procesos, gestionar reclamaciones, etc. Por lo tanto, es algo que ya tenemos de base: una gran cantidad de informaci√≥n por explotar. Lo que propongo con este tutorial es aplicar los LLM al an√°lisis de esos datos para mejorar los procesos de las compa√±√≠as y, por supuesto, la atenci√≥n al cliente.

Veremos c√≥mo, de una manera muy sencilla, podemos procesar las transcripciones de esas conversaciones para obtener datos valiosos que ayuden a mejorar: res√∫menes de las conversaciones, categorizaci√≥n, detecci√≥n del tono de la conversaci√≥n e, incluso, una lista de palabras clave.

## ¬øPor d√≥nde empezamos?

El proyecto est√° organizado siguiendo una estructura muy sencilla, ya que es √∫nicamente una prueba de concepto. Para su uso en producci√≥n, requerir√≠a trabajo adicional, que comentaremos en los √∫ltimos puntos, incluyendo posibles mejoras y aspectos que podemos implementar para acercarnos m√°s al mundo de la IA.

### Estructura del proyecto

La estructura del proyecto es la siguiente:

```
customer-care-llm/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ configuration.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

Esta estructura permite separar los distintos m√≥dulos seg√∫n su responsabilidad, mejorando la escalabilidad y mantenibilidad. Los tests necesarios est√°n incluidos dentro de cada carpeta. A continuaci√≥n, describiremos brevemente el contenido de cada una:

* _app_ aplicaci√≥n general. Todo estar√° contenido dentro de esta carpeta.
* _api_ endpoints de nuestra aplicaci√≥n; ser√° nuestro punto de interacci√≥n con el exterior.
* _models_ representaci√≥n del modelo; define la entrada y salida que usar√° el API.
* _services_ integraci√≥n con los distintos modelos mediante la instanciaci√≥n de servicios y m√©todos para el an√°lisis realizado por los LLM definidos.
* _training_ dado que algunos modelos requieren un peque√±o entrenamiento, aqu√≠ se encuentran las factor√≠as que nos devuelven un modelo entrenado seg√∫n nuestras necesidades, as√≠ como los datos de entrenamiento necesarios.
* _.env_ definici√≥n de variables para la configuraci√≥n de la aplicaci√≥n, como los modelos a instanciar, datos de contacto, etc.
* _requirements.txt_ gesti√≥n de dependencias necesarias para ejecutar nuestra aplicaci√≥n.

Adem√°s el proyecto incluye un README.md con las instrucciones para ejecutar.

### Modelos utilizados

En este proyecto, he utilizado distintos modelos capaces de procesar el lenguaje natural (NLP) para analizar varios aspectos de las conversaciones.
Estos modelos se han seleccionado por su buen rendimiento en tareas de NLP en espa√±ol y su capacidad para procesar el lenguaje en contexto. Todos los modelos est√°n disponibles de forma gratuita en [Hugging Face](https://huggingface.co/).

#### An√°lisis de sentimientos

Utilizo el modelo `nmarinnn/bert-bregman` para determinar el tono emocional de la conversaci√≥n. El resultado ser√° un valor que podemos interpretar como negativo, positivo o neutro.

#### Clasificaci√≥n de texto

Empleo un modelo personalizado basado en `bert-base-spanish-wwm-cased` para categorizar el tipo de incidencia. El entrenamiento del modelo asocia frases de ejemplo con las etiquetas que queremos gestionar, permiti√©ndonos as√≠ establecer las categor√≠as que necesitemos.

#### Generaci√≥n de res√∫menes

Utilizo el modelo `mrm8488/bert-spanish-cased-finetuned-summarization` para crear res√∫menes concisos de las conversaciones. Es importante tener cuidado si el tama√±o del resumen supera al del texto original, ya que puede empezar a alucinar, generando contenido adicional hasta alcanzar el tama√±o deseado.

#### Reconocimiento de entidades nombradas (NER)

Las entidades nombradas son simplemente palabras clave. Para obtener un resultado coherente y significativo, adem√°s de aplicar el modelo `dccuchile/bert-base-spanish-wwm-cased` para extraer palabras clave relevantes, realizo un postproceso con _SpaCy_ para filtrar √∫nicamente sustantivos, adjetivos y adverbios. De esta forma, evitamos que aparezcan palabras como art√≠culos, preposiciones, etc., que no aportan significado.

## Al turr√≥n: analicemos el c√≥digo

### Configuraci√≥n general
<!-- file: app/configuration.py -->
```python

import os
from dotenv import load_dotenv
from pydantic_settings import BaseSettings
import torch

load_dotenv()

class Settings(BaseSettings):
    app_name: str = os.getenv("APP_NAME", "CallCenterAnalysisApp")
    admin_email: str = os.getenv("ADMIN_EMAIL", "admin@example.com")
    items_per_user: int = int(os.getenv("ITEMS_PER_USER", "50"))
    sentiment_model_path: str = os.getenv("SENTIMENT_MODEL_PATH", "nmarinnn/bert-bregman")
    classification_model_path: str = os.getenv("CLASSIFICATION_MODEL_PATH", "bert-base-spanish-wwm-cased")
    summarization_model_path: str = os.getenv("SUMMARIZATION_MODEL_PATH", "mrm8488/bert-spanish-cased-finetuned-summarization")
    ner_model_path: str = os.getenv("NER_MODEL_PATH", "dccuchile/bert-base-spanish-wwm-cased")

    class Config:
        env_file = ".env"

settings = Settings()

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

MODEL_PATHS = {
    "sentiment": settings.sentiment_model_path,
    "classification": settings.classification_model_path,
    "summarization": settings.summarization_model_path,
    "ner": settings.ner_model_path
}
```

Aqu√≠ definimos las configuraciones globales, como los modelos a utilizar, el uso de CUDA o CPU para la ejecuci√≥n, etc. De esta forma, ganamos flexibilidad y mantenemos un √∫nico punto de gesti√≥n para los datos variables. Esto se puede mejorar a√±adiendo par√°metros adicionales para la configuraci√≥n de los LLM. Todos los valores se pueden especificar como variables de entorno o definir en un archivo .env, tal como se indica en el README del proyecto.

### Servicios de an√°lisis

<!-- file: app/services/sentiment_analysis.py -->
```python
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

from configuration import DEVICE, settings

class SentimentAnalysisService:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(settings.sentiment_model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(settings.sentiment_model_path).to(DEVICE)
        self.pipeline = pipeline("sentiment-analysis", model=self.model, tokenizer=self.tokenizer, device=DEVICE)

    def analyze(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)

        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(probabilities, dim=-1).item()

        class_labels = {0: "negativo", 1: "neutro", 2: "positivo"}
        return class_labels[predicted_class]
```

El modelo est√° preentrenado, por lo que solo necesitamos enviarle el texto para que lo analice y luego traducir su respuesta a un formato legible.

<!-- file: app/services/text_classification.py -->
```python
import torch
from transformers import pipeline, AutoTokenizer

from configuration import DEVICE, settings, TEXT_CLASSIFICATION_CATEGORIES
from training.text_classification_factory import train_model

class TextClassificationService:

    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(settings.text_classification_model_path)
        self.model = train_model()
        self.pipeline = pipeline("text-classification", model=self.model, tokenizer=self.tokenizer, device=DEVICE)

    def classify(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        outputs = self.model(**inputs)
        predicted_class = torch.argmax(outputs.logits, dim=1).item()
        return TEXT_CLASSIFICATION_CATEGORIES[predicted_class]
```

A diferencia del servicio anterior, aqu√≠ necesitamos realizar un peque√±o entrenamiento del modelo para que nos devuelva etiquetas significativas para nuestro caso espec√≠fico. Esto requiere indicarle c√≥mo relacionar tipos de frases con un conjunto de etiquetas definido. Esta parte es especialmente potente, ya que nos permite categorizar las conversaciones de una forma no preestablecida por el LLM.

<!-- file: app/services/extractive_summary.py -->
```python
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

from configuration import DEVICE, settings

class ExtractiveSummaryService:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(settings.extractive_summary_model_path)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(settings.extractive_summary_model_path).to(DEVICE)
        self.pipeline = pipeline("summarization", model=self.model, tokenizer=self.tokenizer, device=DEVICE)

    def summarize(self, text):
        result = self.pipeline(text, max_length=100, min_length=30, do_sample=False)
        return result[0]['summary_text']
```

Quiz√° el m√°s sencillo de todos. √önicamente tenemos que enviarle el texto y las longitudes que queremos como l√≠mite.

<!-- file: app/services/named_entity_recognition.py -->
```python
import spacy
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
from configuration import DEVICE, settings
from collections import Counter

class NamedEntityRecognitionService:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(settings.ner_model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(settings.ner_model_path).to(DEVICE)
        self.pipeline = pipeline("ner", model=self.model, tokenizer=self.tokenizer, device=DEVICE)
        self.nlp = spacy.load("es_core_news_sm")

    def extract_keywords(self, text):
        results = self.pipeline(text)

        # Filtrar y limpiar las entidades reconocidas
        entities = [item['word'].strip('#').lower() for item in results if item['score'] > 0.5 and len(item['word']) > 1]

        # An√°lisis morfol√≥gico para restringir a sustantivos, adjetivos y verbos
        doc = self.nlp(text)
        valid_pos = {'NOUN', 'ADJ', 'VERB'}
        filtered_entities = [
            token.text.lower() for token in doc
            if token.pos_ in valid_pos and token.text.lower() not in entities
        ]

        # Contar las ocurrencias de cada entidad
        entity_counts = Counter(filtered_entities)

        # Seleccionar las 5 entidades m√°s frecuentes como palabras clave
        keywords = [word for word, _ in entity_counts.most_common(5)]
        return keywords
```

Es el √∫nico servicio que contiene algo de l√≥gica de procesamiento fuera de los LLM. En este caso, el LLM identificar√° palabras con una determinada importancia (proceso NER), y podremos filtrarlas seg√∫n esa puntuaci√≥n. Para evitar que aparezcan t√©rminos sin significado concreto, nos apoyamos en SpaCy para procesar esa lista y quedarnos √∫nicamente con las palabras que cumplan el criterio de ser sustantivos, adjetivos o adverbios mediante el an√°lisis morfol√≥gico. Este filtro es tambi√©n configurable y, como se puede ver, bastante sencillo de implementar. Por lo tanto, solo aquellas palabras que se encuentren en ambas listas ser√°n aptas para considerarse palabras clave de la conversaci√≥n.


<!-- file: app/api/endpoints.py -->
```python
from fastapi import APIRouter, HTTPException
from models.schemas import CallRequest, CallAnalysis
from services.sentiment_analysis import SentimentAnalysisService
from services.text_classification import TextClassificationService
from services.extractive_summary import ExtractiveSummaryService
from services.named_entity_recognition import NamedEntityRecognitionService

router = APIRouter()

# Instancias de servicios
sentiment_service = SentimentAnalysisService()
classification_service = TextClassificationService()
summarization_service = ExtractiveSummaryService()
ner_service = NamedEntityRecognitionService()

@router.post("/analyze_call", response_model=CallAnalysis)
async def analyze_call(call: CallRequest) -> CallAnalysis:
    try:
        # Procesar la transcripci√≥n
        transcription = call.transcription
        sentiment = sentiment_service.analyze(transcription)
        category = classification_service.classify(transcription)
        summary = summarization_service.summarize(transcription)
        keywords = ner_service.extract_keywords(transcription)

        # Retornar el an√°lisis de la llamada
        return CallAnalysis(
            transcription=call.transcription,
            sentiment=sentiment,
            category=category,
            summary=summary,
            keywords=keywords
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

Aqu√≠ estamos definiendo una API muy sencilla, con un √∫nico endpoint POST en el que podemos proporcionar la transcripci√≥n de la conversaci√≥n y obtener como respuesta su an√°lisis. A continuaci√≥n, se muestra un peque√±o ejemplo de una petici√≥n y su respuesta:

<!-- file: request.json -->
```json
{
  "transcription": "Buenas tardes, querr√≠a informaci√≥n sobre los productos de inversi√≥n que puedo contratar, dado mi historial crediticio y de solvencia. Estar√≠a interesado en productos de bajo riesgo, con una rentabilidad moderada y que permitan aportaciones peri√≥dicas. ¬øTienen alg√∫n tipo de producto que haga uso del inter√©s compuesto? ¬øMe pueden asegurar si los sectores en los que invierten no incluyen armamento ni actividades moralmente cuestionables? Gracias."
}
```

<!-- file: response.json -->
```json
{
  "transcription": "Buenas tardes, querr√≠a informaci√≥n sobre los productos de inversi√≥n que puedo contratar, dado mi historial crediticio y de solvencia. Estar√≠a interesado en productos de bajo riesgo, con una rentabilidad moderada y que permitan aportaciones peri√≥dicas. ¬øTienen alg√∫n tipo de producto que haga uso del inter√©s compuesto? ¬øMe pueden asegurar si los sectores en los que invierten no incluyen armamento ni actividades moralmente cuestionables? Gracias.",
  "sentiment": "positivo",
  "category": "Solicitud de Servicio",
  "summary": "Buenas tardes, querr√≠a informaci√≥n sobre los productos de inversi√≥n que puedo contratar dado mi historial crediticio y de solvencia. Estar√≠a interesado en productos de bajo riesgo, con una rentabilidad moderada y que permitan aportaciones peri√≥dicas. ¬øTienen alg√∫n tipo de producto que haga uso del inter√©s compuesto? ¬øMe pueden asegurar si los sectores",
  "keywords": [
    "crediticio",
    "solvencia",
    "inversi√≥n",
    "cuestionables"
  ]
}
```

Podemos observar que, al tratarse de un texto corto y de un modelo preentrenado sin ajustes espec√≠ficos, el resumen no es de gran calidad. Por supuesto, existen alternativas de pago mucho m√°s fiables y, dedicando tiempo y cuidado, tambi√©n es posible obtener buenos resultados con modelos gratuitos.

## Conclusiones

Como podemos ver, no todo es humo en el mundo de la IA. Hay aplicaciones que, de forma tradicional, nos costar√≠an much√≠simo m√°s, tanto en trabajo como en mantenibilidad. Es importante saber identificar los casos de uso donde estas soluciones aportan valor y aprovecharlos sin dudar.

Aunque este proyecto no es apto para un sistema real, s√≠ puede servirnos como punto de partida para plantearnos lo que podr√≠amos llegar a conseguir integrando este tipo de soluciones en el portafolio de nuestra compa√±√≠a. Los pr√≥ximos pasos a seguir nos pueden dar una idea de hasta qu√© punto podemos mejorar esta soluci√≥n y aproximarnos a un sistema que aproveche la potencia que los LLM ofrecen para optimizar tanto nuestros procesos como la relaci√≥n que mantenemos con los clientes.


### Posibles pr√≥ximos pasos

* Implementar un sistema de logging para rastrear el uso de la API, monitorear el rendimiento y detectar errores.
* A√±adir autenticaci√≥n y autorizaci√≥n: todas nuestras aplicaciones deben estar correctamente protegidas frente a un uso fraudulento.
* Integraci√≥n con aplicaciones CRM existentes: si como parte del flujo alimentamos el sistema, podremos tener en tiempo real (NRT) un an√°lisis de las conversaciones a disposici√≥n de los agentes.
* Implementar retroalimentaci√≥n para permitir la correcci√≥n o mejora de las predicciones del modelo.
* Expandir las capacidades de an√°lisis: mejorar la detecci√≥n de emociones espec√≠ficas, identificaci√≥n de problemas recurrentes, etc.
* Optimizaci√≥n del rendimiento: existen modelos m√°s ligeros que pueden reducir la latencia en las respuestas, t√©cnicas de inferencia m√°s r√°pida, uso de otros modelos, etc.
* Implementar pruebas de carga para evaluar el rendimiento real del sistema y su capacidad de an√°lisis.
* Explotaci√≥n de los resultados: almacenar las respuestas en una base de datos para el an√°lisis de patrones, creaci√≥n de cuadros de mando, etc.

Hay multitud de posibilidades para hacer crecer el proyecto, y seguro que si le preguntamos a ChatGPT, nos puede sugerir alguna m√°s üòâ.

## Glosario de t√©rminos

ü§ñ_generated by IA_
* LLM (Large Language Model): Un tipo de modelo de inteligencia artificial especializado en el procesamiento de lenguaje natural y la generaci√≥n de texto.
* Trazabilidad: Capacidad de rastrear y verificar el historial, uso o localizaci√≥n de un producto o informaci√≥n en un proceso.
* EndPoints: Puntos de acceso en una API (Interfaz de Programaci√≥n de Aplicaciones) que permiten la comunicaci√≥n entre diferentes sistemas.
* Pipeline: En el contexto de aprendizaje autom√°tico, una serie de pasos o transformaciones aplicadas a los datos para entrenar o utilizar un modelo.
* CUDA: Una plataforma de computaci√≥n paralela desarrollada por NVIDIA, que permite utilizar las GPU para procesamiento en lugar de la CPU.
* API Router: Un sistema de rutas en una API que organiza y redirige solicitudes a diferentes endpoints seg√∫n la funcionalidad requerida.
* Modelo BERT: Un tipo de modelo de aprendizaje autom√°tico para procesamiento de lenguaje natural que permite entender el contexto completo de una palabra en una oraci√≥n.
* Pydantic: Biblioteca de validaci√≥n de datos para Python que facilita la creaci√≥n de modelos de datos basados en clases.
* SpaCy: Herramienta avanzada de procesamiento de lenguaje natural (NLP) en Python, utilizada para an√°lisis l√©xico y morfol√≥gico del texto.
* NER (Named Entity Recognition): T√©cnica de procesamiento de lenguaje natural que permite identificar nombres de personas, organizaciones, lugares, etc., en un texto.
* Postprocesamiento: Paso en el procesamiento de datos que se realiza despu√©s de la ejecuci√≥n de un modelo, para mejorar o ajustar los resultados obtenidos.
* Softmax: Funci√≥n matem√°tica utilizada en modelos de aprendizaje autom√°tico para normalizar valores en probabilidades.
* Inferencia: En el contexto de IA, proceso de aplicar un modelo entrenado para hacer predicciones o clasificaciones en datos nuevos.
* Retroalimentaci√≥n (Feedback): Proceso de ajuste o mejora continua mediante el an√°lisis de resultados o respuestas, especialmente importante en el entrenamiento de modelos de IA.
* NRT (Near Real-Time): Procesamiento casi en tiempo real, permitiendo obtener datos o resultados poco despu√©s de la entrada de datos.

## Referencias

* [C√≥digo del tutorial en GitLab](https://gitlab.com/tutoriales-ysegura/customer-care-llm)
* [Hugging Face](https://huggingface.co/)